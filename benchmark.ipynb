{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-source LLM's as Agents with `ChatHuggingFace`\n",
    "\n",
    "\n",
    "Open source LLMs are becoming viable general purpose agents. The goal of this notebook is to demonstrate how to make use of open-source LLMs as chat models using [Hugging Face Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) with [LangChain's `ChatHuggingFace`]() to enable their usage and experimentation with agent-based workflows.\n",
    "\n",
    "In particular, we will:\n",
    "1. Utilize the [HuggingFaceEndpoint](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/huggingface_endpoint.py) (or [HuggingFaceTextGenInference](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/huggingface_text_gen_inference.py) or [HuggingFaceHub](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/huggingface_hub.py)) integration to call a [HF Inference Endpoint](https://huggingface.co/inference-endpoints) that's serving an LLM via [Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/index)\n",
    "2. Utilize the `ChatHuggingFace` class that interfaces between LangChain's [Chat Messages](https://python.langchain.com/docs/modules/model_io/chat/#messages) and the hosted LLM by leveraging [Hugging Face's Chat Templates](https://huggingface.co/docs/transformers/chat_templating) to power a `ChatAgent` pipeline.\n",
    "4. Demonstrate how to use an open-source LLM in a zero-shot ReAct Agent workflow, along with an open-source [Prometheus](https://huggingface.co/papers/2310.08491) model to perform \"LLM as a Judge\"-style evaluations on that Agent's ouputs.\n",
    "5. Understand how several different open-source LLM's perform as general purpose agents by running an asynchronous evaluation pipeline using Prometheus as the judge. \n",
    "\n",
    "\n",
    "\n",
    "> Note: To run this notebook, you'll need to have:\n",
    "> - an LLM deployed via a Hugging Face Inference Endpoint (the LLM must have a `chat_template` defined in its `tokenizer_config.json`)\n",
    "> - A Hugging Face Token with access to the deployed endpoint saved as an environment variable: `HUGGINGFACEHUB_API_TOKEN`\n",
    "> - A SerpAPI key saved as an environment variable: `SERPAPI_API_KEY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers langchain text-generation python-dotenv jinja2 langchainhub numexpr datasets tqdm openai sentencepiece protobuf matplotlib wikipedia google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.output_parsers import (\n",
    "    ReActJsonSingleInputOutputParser,\n",
    ")\n",
    "from prompts import SYSTEM_PROMPT, HUMAN_PROMPT\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.tools import WikipediaQueryRun, tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import load_tools\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate an LLM\n",
    "\n",
    "You'll need to have a running Inference Endpoint available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `HuggingFaceHub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id='HuggingFaceH4/zephyr-7b-beta',\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `HuggingFaceEndpoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceEndpoint\n",
    "\n",
    "endpoint_url = \"https://j02macig8fbovqhr.us-east-1.aws.endpoints.huggingface.cloud\"  # zephyr\n",
    "endpoint_url = 'https://iw8z8uxlp03gvuxc.us-east-1.aws.endpoints.huggingface.cloud'  # mixtral\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=endpoint_url,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 488,\n",
    "        \"top_k\": 50,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a wrapper for `BaseChatModel` to apply chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from chat_wrapper import HuggingFaceChatWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model and some messages to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"You're a helpful assistant. What happens when an unstoppable force meets an immovable object?\"),\n",
    "]\n",
    "\n",
    "chat_model = HuggingFaceChatWrapper(llm=llm)\n",
    "chat_model.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat_model.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Here we'll test out our model as a zero-shot ReAct Agent. The example below is taken from [here](https://python.langchain.com/docs/modules/agents/agent_types/react#using-chat-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the agent with a `react-json` style prompt and access to a search engine and calculator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "# Rename tools in the same format used by other tools\n",
    "tools[0].name = \"search\"\n",
    "tools[1].name = \"calculator\"\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Searches Wikipedia for a query. This will not be relevant for the latest information, but it can be useful for historical knowledge.\"\"\"\n",
    "    return wikipedia.run(query)\n",
    "\n",
    "\n",
    "TOOLS = tools + [search_wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(SYSTEM_PROMPT+'\\nSo, here is my question:'+HUMAN_PROMPT),\n",
    "    ]\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    tool_description=render_text_description_and_args(TOOLS),\n",
    "    tool_names=\", \".join([t.name for t in TOOLS]),\n",
    ")\n",
    "\n",
    "# define the agent\n",
    "chat_model_with_stop = chat_model.bind(stop=[\"\\nObservation\"])\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model_with_stop\n",
    "    | ReActJsonSingleInputOutputParser()\n",
    ")\n",
    "\n",
    "# instantiate AgentExecutor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=TOOLS,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    \"input\": \"What is the age of Leonardo DiCaprio's current girlfriend, raised to the power 0.43?\"\n",
    "}\n",
    "\n",
    "out = agent_executor.invoke(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Experiment \n",
    "\n",
    "- Find a group of test questions that use a certain set of tools to solve (HotpotQA)\n",
    "- Run several different models as agent to solve the test questions (OS and proprietary)\n",
    "- Use a LLM as a judge\n",
    "- Use OS models as a judge\n",
    "- Report correlations\n",
    "\n",
    "## Create evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "hotpotqa_dataset = load_dataset(\"hotpot_qa\", \"distractor\")\n",
    "\n",
    "# let's sample a few examples from each level (of difficulty) and type (comparion or bridge)\n",
    "hotpotqa_dataset.set_format(\"pandas\")\n",
    "dataset_df = hotpotqa_dataset[\"train\"][:]\n",
    "sample_indicies = (\n",
    "    dataset_df.groupby([\"level\", \"type\"]).sample(4, random_state=10).index.values\n",
    ")\n",
    "hotpotqa_dataset.reset_format()\n",
    "hotpotqa_dataset = hotpotqa_dataset[\"train\"].select(sample_indicies)\n",
    "task_column = [f\"HotpotQA-{level}\" for level in hotpotqa_dataset[\"level\"]]\n",
    "hotpotqa_dataset = hotpotqa_dataset.add_column('task', task_column).select_columns(['question', 'answer', 'task'])\n",
    "pd.DataFrame(hotpotqa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "math_dataset = load_dataset(\"gsm8k\", \"main\")['train']\n",
    "math_dataset = math_dataset.select(np.random.randint(0, len(math_dataset), 15))\n",
    "task_column = [\"GSM8K\"] * len(math_dataset)\n",
    "math_dataset = math_dataset.add_column('task', task_column).select_columns(['question', 'answer', 'task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_dataset = load_dataset(\"gaia-benchmark/GAIA\", \"2023_level1\")['validation']\n",
    "gaia_dataset.set_format('pandas')\n",
    "gaia_dataset_df = gaia_dataset[:]\n",
    "gaia_dataset_df['number_of_steps'] = gaia_dataset_df['Annotator Metadata'].apply(lambda row: int(row['Number of steps']))\n",
    "gaia_dataset_df['tools_used'] = gaia_dataset_df['Annotator Metadata'].apply(lambda row: row['Tools'])\n",
    "gaia_dataset_df = gaia_dataset_df.loc[~gaia_dataset_df['tools_used'].str.lower().str.contains('pdf|excel|image|video|parsing|audio|word|file|speech|viewer|markdown|python|editor')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indicies = [1, 18, 23, 29, 39, 42, 47, 49, 50, 52]\n",
    "gaia_dataset = gaia_dataset.rename_columns({'Question': 'question', 'Final answer': 'answer'}).select_columns(['question', 'answer'])\n",
    "gaia_dataset.reset_format()\n",
    "gaia_dataset = gaia_dataset.select(selected_indicies)\n",
    "\n",
    "task_column = ['GAIA'] * len(gaia_dataset)\n",
    "gaia_dataset = gaia_dataset.add_column('task', task_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([math_dataset, hotpotqa_dataset, gaia_dataset])\n",
    "pd.DataFrame(dataset).groupby('task').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_agents import run_full_tests, build_hf_agent, build_openai_agent\n",
    "\n",
    "agent_endpoints = {\n",
    "    # 'Zephyr-7b-beta': 'https://n6c9uxjetjnmi44q.us-east-1.aws.endpoints.huggingface.cloud',\n",
    "    # 'https://i45e7q2do4r8mw5k.us-east-1.aws.endpoints.huggingface.cloud',  # notus-7b\n",
    "    'Mixtral-7x8b': 'https://iw8z8uxlp03gvuxc.us-east-1.aws.endpoints.huggingface.cloud',\n",
    "    # 'https://h0qwp3dx2iixajoh.us-east-1.aws.endpoints.huggingface.cloud', # llama2-7b\n",
    "    # 'Yi-34B-Chat': 'https://wtgjpwu76xh7cv7t.us-east-1.aws.endpoints.huggingface.cloud',\n",
    "    # 'Llama2-7b': 'https://pbg28xzbho42zp1t.us-east-1.aws.endpoints.huggingface.cloud',\n",
    "    # 'SOLAR-10.7B': 'https://dxsuz0i09l5zzjh1.us-east-1.aws.endpoints.huggingface.cloud'\n",
    "}\n",
    "\n",
    "agents = {\n",
    "    name: build_hf_agent(endpoint)\n",
    "    for name, endpoint in agent_endpoints.items()\n",
    "}\n",
    "\n",
    "# uncomment below to test GPT4 as an agent\n",
    "agents['GPT4'] = build_openai_agent(model_id='gpt-4-1106-preview')\n",
    "# agents['GPT3.5'] = build_openai_agent(model_id='gpt-3.5-turbo-1106')\n",
    "print(agents)\n",
    "# run eval\n",
    "await run_full_tests(\n",
    "    dataset=dataset,\n",
    "    agents=agents\n",
    ")\n",
    "print('Evaluation complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from evaluation import build_evaluator\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "eval_model_name = \"GPT4\"\n",
    "\n",
    "# prometheus_endpoint = 'https://xe7njj8b9w43222g.us-east-1.aws.endpoints.huggingface.cloud'\n",
    "# eval_chat_model = build_evaluator(prometheus_endpoint)\n",
    "# eval_model_name = \"Prometheus-13B-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chat_model.invoke(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_answers\n",
    "from prompts import CORRECTNESS_PROMPT_TEMPLATE\n",
    "\n",
    "for file in tqdm(glob.glob(\"output/*.json\")):\n",
    "    evaluate_answers(\n",
    "        file,\n",
    "        eval_chat_model,\n",
    "        eval_model_name,\n",
    "        CORRECTNESS_PROMPT_TEMPLATE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for file in glob.glob(\"output/*.json\"):\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        results.append(pd.read_json(f))\n",
    "\n",
    "result_df = pd.concat(results)\n",
    "result_df['tools_used'] = result_df['intermediate_steps'].apply(lambda row: ([step['tool'] for step in row] if row is not None else None))\n",
    "result_df['number_of_distinct_tools_used'] = result_df['tools_used'].apply(lambda row: (len(list(set(row))) if row is not None else None))\n",
    "result_df['number_of_steps'] = result_df['tools_used'].apply(lambda row: (len(row) if row is not None else None))\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_result(x):\n",
    "    try:\n",
    "        return int(x) - 1\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "result_df[f'eval_score_{eval_model_name}'] = result_df[f'eval_score_{eval_model_name}'].apply(interpret_result)\n",
    "\n",
    "result_df.groupby(\"agent_name\").agg(\n",
    "    {f'eval_score_{eval_model_name}': \"mean\", \"parsing_error\": \"sum\", \"iteration_limit_exceeded\": \"sum\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = (\n",
    "    result_df.groupby([\"agent_name\", \"task\"])\n",
    "    .agg({f\"eval_score_{eval_model_name}\": \"mean\"})\n",
    "    .unstack(\"task\")\n",
    ")\n",
    "display(agg_df)\n",
    "\n",
    "ax = agg_df[\n",
    "    [\n",
    "        (f\"eval_score_{eval_model_name}\", \"HotpotQA-easy\"),\n",
    "        (f\"eval_score_{eval_model_name}\", \"HotpotQA-medium\"),\n",
    "        (f\"eval_score_{eval_model_name}\", \"HotpotQA-hard\"),\n",
    "        (f\"eval_score_{eval_model_name}\", \"GSM8K\"),\n",
    "        (f\"eval_score_{eval_model_name}\", \"GAIA\")\n",
    "    ]\n",
    "].plot.bar(\n",
    "    figsize=(10, 5),\n",
    "    title=f\"Mean {eval_model_name} Evaluation Score by Task\",\n",
    "    xlabel=\"Model\",\n",
    "    ylabel=\"Mean Evaluation Score\",\n",
    ")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() * 1.01, p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizations to make\n",
    "- avg model score by question difficulty\n",
    "- count of parsing errors and iteration limit errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do:\n",
    "- Generalize evaluation code\n",
    "- Make evals fully async\n",
    "- Select 3-5 models to evaluate on\n",
    "- Run eval\n",
    "- Write blog post\n",
    "- Get LangChain PR merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- One of the main challenges with open LLMs is ensuring they adhere to the proper markdown JSON output!\n",
    "- Another is when they serve as the evaluator, they struggle to handle the default \"labeled_criteria\" prompt and end up hallucinating. Needs to be modified / simplified.\n",
    "- I also found that GPT4 actually did pretty poor at judging correctness given the default LangChain prompt!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
